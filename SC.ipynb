{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from config import base_config\n",
    "from sklearn.metrics import classification_report\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'  # 设置环境变量\n",
    "import random\n",
    "#设定随机种子\n",
    "torch.manual_seed(9)\n",
    "# 检查是否有可用的GPU设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "def read_json(dataset_path,label_space,sample_num=-1):\n",
    "    \"\"\"\n",
    "    input: dataset_path: str\n",
    "    output: ture_confidence_list, predict_confidence_list \n",
    "    len(all_data) * seq_len * len(label_space)\n",
    "    在AIGA数据集中为len(data) * k-shot+1 * 2\n",
    "    \"\"\"\n",
    "    label_index = {label: idx for idx, label in enumerate(label_space)}\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = f.readlines()\n",
    "        if sample_num != -1:\n",
    "            all_data = random.sample(all_data, sample_num)\n",
    "        all_ture_confidence_list = []\n",
    "        all_predict_confidence_list = []\n",
    "        all_prior_confidences = []\n",
    "        all_cc_prior_confidences = []\n",
    "        all_LinC = []\n",
    "        for line in all_data:\n",
    "            data = json.loads(line)\n",
    "            # calculate true confidence\n",
    "            label_distribution = data['label_distribution']\n",
    "            true_confidence_list = [] # seq_len * num_labels\n",
    "            for label in label_distribution:\n",
    "                true_confidence = [0] * len(label_space)\n",
    "                true_confidence[label_index[label]] = 1\n",
    "                true_confidence_list.append(true_confidence)\n",
    "            # calculate predict confidence\n",
    "            label_token_confidences = data['label_token_confidences']\n",
    "            predict_confidence_list = [] # seq_len * num_labels\n",
    "            for k, v in label_token_confidences.items():\n",
    "                label_token_confidence = list(v.values())\n",
    "                #使confidence和为1\n",
    "                label_token_confidence = [i/sum(label_token_confidence) for i in label_token_confidence]\n",
    "                predict_confidence_list.append(label_token_confidence)\n",
    "            all_ture_confidence_list.append(true_confidence_list)\n",
    "            all_predict_confidence_list.append(predict_confidence_list)\n",
    "\n",
    "            try:\n",
    "                prior_confidences = list(data['prior_confidences'].values())\n",
    "                all_prior_confidences.append(prior_confidences)\n",
    "                cc_prior_confidences = list(data['cc_prior_confidences'].values())\n",
    "                all_cc_prior_confidences.append(cc_prior_confidences)\n",
    "                LinC_pram = data['LinC_pram']['weight']\n",
    "                LinC_pram =[abs(i) for i in LinC_pram]\n",
    "                all_LinC.append(LinC_pram)\n",
    "            except:\n",
    "                all_prior_confidences = []\n",
    "                all_cc_prior_confidences = []\n",
    "                all_LinC = []\n",
    "    return all_ture_confidence_list, all_predict_confidence_list, all_prior_confidences, all_cc_prior_confidences, all_LinC\n",
    "\n",
    "def batch_calibrate(all_ture_confidence_list,all_predict_confidence_list,all_prior_confidence,label_space):\n",
    "    all_ture_label = []\n",
    "    for ture_confidence_list in all_ture_confidence_list:\n",
    "        ture_label = ture_confidence_list[-1]\n",
    "        ture_label = max(range(len(label_space)), key=lambda i: ture_label[i])\n",
    "        all_ture_label.append(ture_label)\n",
    "        \n",
    "    all_predict_confidence = []\n",
    "    for confidence_list in all_predict_confidence_list:\n",
    "        confidence = confidence_list[-1]\n",
    "        all_predict_confidence.append(confidence)\n",
    "    \n",
    "    # all_ture_label: len(data) * num_labels\n",
    "    # all_predict_confidence: len(data) * num_labels\n",
    "\n",
    "    # 计算校准前的标签\n",
    "    all_predict_label = [] #len(data) * num_labels 独热向量\n",
    "    for confidence in all_predict_confidence:\n",
    "        predict_label = max(range(len(label_space)), key=lambda i: confidence[i])\n",
    "        all_predict_label.append(predict_label)\n",
    "    \n",
    "    # 计算classification report\n",
    "    #report = classification_report(all_ture_label, all_predict_label, target_names=label_space)\n",
    "    accuracy = sum(1 for l, p in zip(all_ture_label, all_predict_label) if l == p) / len(all_ture_label)\n",
    "    #print('校准前的准确率：', accuracy)\n",
    "\n",
    "    # 计算每个类别的平均置信度\n",
    "    num_labels = len(label_space)\n",
    "    mean_confidence = [0] * num_labels\n",
    "    for i in range(num_labels):\n",
    "        mean_confidence[i] = sum([confidence[i] for confidence in all_predict_confidence]) / len(all_predict_confidence)\n",
    "\n",
    "    # 计算校准后的置信度：校准前置信度/平均置信度\n",
    "    all_calibrate_confidence = []\n",
    "    for confidence in all_predict_confidence:\n",
    "        calibrate_confidence = [confidence[i] / mean_confidence[i] for i in range(num_labels)]\n",
    "        all_calibrate_confidence.append(calibrate_confidence)\n",
    "    \n",
    "    # 计算校准后的标签\n",
    "    all_calibrate_label = []\n",
    "    for confidence in all_calibrate_confidence:\n",
    "        calibrate_label = max(range(num_labels), key=lambda i: confidence[i])\n",
    "        all_calibrate_label.append(calibrate_label)\n",
    "\n",
    "    # 计算classification report\n",
    "    #report = classification_report(all_ture_label, all_calibrate_label, target_names=label_space)\n",
    "    accuracy_a = sum(1 for l, p in zip(all_ture_label, all_calibrate_label) if l == p) / len(all_ture_label)\n",
    "    #print('BC校准后的准确率：', accuracy_a)\n",
    "\n",
    "    prior_calibrate_confidence = []\n",
    "    for confidence,prior_confidence in zip(all_predict_confidence,all_prior_confidence):\n",
    "        calibrate_confidence = [confidence[i] / prior_confidence[i] for i in range(num_labels)]\n",
    "        prior_calibrate_confidence.append(calibrate_confidence)\n",
    "\n",
    "    prior_calibrate_label = []\n",
    "    for confidence in prior_calibrate_confidence:\n",
    "        calibrate_label = max(range(num_labels), key=lambda i: confidence[i])\n",
    "        prior_calibrate_label.append(calibrate_label)\n",
    "    accuracy_b = sum(1 for l, p in zip(all_ture_label, prior_calibrate_label) if l == p) / len(all_ture_label)\n",
    "    #print('10-BC校准后的准确率：', accuracy_b)\n",
    "\n",
    "    return accuracy,accuracy_a, accuracy_b,prior_calibrate_confidence\n",
    "\n",
    "def multiclass_pointwise_surprise(prior, true_label_onehot, eps=0):\n",
    "    \"\"\"\n",
    "    prior: torch.Tensor, (batch_size, seq_len, num_class)，概率分布\n",
    "    true_label_onehot: torch.Tensor, (batch_size, seq_len, num_class)，one-hot向量\n",
    "    return: torch.Tensor, (batch_size, seq_len, num_class), 每个类别的surprise分数\n",
    "    \"\"\"\n",
    "    # 避免 log(0) 出现 nan\n",
    "    #surprise_true = torch.log(1.0 / (prior + eps))\n",
    "    surprise_true = 1\n",
    "    #surprise_false = torch.log(1.0 - prior + eps)\n",
    "    surprise_false = -surprise_true\n",
    "    surprise = true_label_onehot * surprise_true + (1 - true_label_onehot) * surprise_false\n",
    "    return surprise\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def data_reader(all_true_confidence_train, all_predict_confidence_train,\n",
    "               all_true_confidence_test, all_predict_confidence_test):\n",
    "    \"\"\"\n",
    "    读取并处理数据，分离支持示例和查询示例。\n",
    "\n",
    "    参数:\n",
    "        all_true_confidence_train (list or np.array): 训练集真实置信度(ture label)，形状 (num_train, k+1, num_labels)\n",
    "        all_predict_confidence_train (list or np.array): 训练集预测置信度，形状 (num_train, k+1, num_labels)\n",
    "        all_true_confidence_test (list or np.array): 测试集真实置信度，形状 (num_test, k+1, num_labels)\n",
    "        all_predict_confidence_test (list or np.array): 测试集预测置信度，形状 (num_test, k+1, num_labels)\n",
    "        k (int): 支持示例的数量\n",
    "\n",
    "    返回:\n",
    "        train_loader, test_loader: 训练集和测试集的数据加载器\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #k是第二维度的长度-1\n",
    "    k = len(all_true_confidence_train[0]) - 1\n",
    "\n",
    "    all_true_confidence_train = torch.tensor(all_true_confidence_train, dtype=torch.float32).to(device)\n",
    "    all_predict_confidence_train = torch.tensor(all_predict_confidence_train, dtype=torch.float32).to(device)\n",
    "    all_true_confidence_test = torch.tensor(all_true_confidence_test, dtype=torch.float32).to(device)\n",
    "    all_predict_confidence_test = torch.tensor(all_predict_confidence_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 计算 delta_confidence\n",
    "    delta_confidence_train = all_true_confidence_train - all_predict_confidence_train\n",
    "    delta_confidence_test = all_true_confidence_test - all_predict_confidence_test\n",
    "    # delta_confidence_train = all_true_confidence_train \n",
    "    # delta_confidence_test = all_true_confidence_test \n",
    "    #计算每个类别的surprise分数，形状为 (num_train, k+1, num_labels)\n",
    "    surprise_train = multiclass_pointwise_surprise(all_predict_confidence_train, all_true_confidence_train)\n",
    "    surprise_test = multiclass_pointwise_surprise(all_predict_confidence_test, all_true_confidence_test)\n",
    "\n",
    "    # 分离支持示例和查询示例\n",
    "    support_surprise_train = surprise_train[:, :k, :]  # 形状: (num_train, k, num_labels)\n",
    "    query_surprise_train = surprise_train[:, k, :]     # 形状: (num_train, num_labels)\n",
    "    support_deltas_train = delta_confidence_train[:, :k, :]  # 形状: (num_train, k, num_labels)\n",
    "    query_deltas_train = delta_confidence_train[:, k, :]     # 形状: (num_train, num_labels)\n",
    "    p_pred_query_train = all_predict_confidence_train[:, k, :]  # 形状: (num_train, num_labels)\n",
    "    p_true_query_train = all_true_confidence_train[:, k, :]     # 形状: (num_train, num_labels)\n",
    "\n",
    "    support_surprise_test = surprise_test[:, :k, :]     # 形状: (num_test, k, num_labels)\n",
    "    query_surprise_test = surprise_test[:, k, :]        # 形状: (num_test, num_labels)\n",
    "    support_deltas_test = delta_confidence_test[:, :k, :]  # 形状: (num_test, k, num_labels)\n",
    "    query_deltas_test = delta_confidence_test[:, k, :]     # 形状: (num_test, num_labels)\n",
    "    p_pred_query_test = all_predict_confidence_test[:, k, :]  # 形状: (num_test, num_labels)\n",
    "    p_true_query_test = all_true_confidence_test[:, k, :]     # 形状: (num_test, num_labels)\n",
    "\n",
    "    # 创建训练集和测试集\n",
    "    train_dataset = TensorDataset(support_surprise_train, p_pred_query_train, query_surprise_train, p_true_query_train)\n",
    "    test_dataset = TensorDataset(support_surprise_test, p_pred_query_test, query_surprise_test, p_true_query_test)\n",
    "    # train_dataset = TensorDataset(support_deltas_train, p_pred_query_train, query_deltas_train, p_true_query_train)\n",
    "    # test_dataset = TensorDataset(support_deltas_test, p_pred_query_test, query_deltas_test, p_true_query_test)\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=15000, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Transformer_encoder(nn.Module):\n",
    "    def __init__(self, num_labels, embed_dim=768, num_heads=16, num_layers=4, dropout=0):\n",
    "        \"\"\"\n",
    "        层级递归映射校准模型\n",
    "\n",
    "        参数:\n",
    "            num_labels (int): 标签的数量\n",
    "            embed_dim (int): 嵌入维度\n",
    "            num_heads (int): Transformer 的头数\n",
    "            num_layers (int): Transformer 编码器的层数\n",
    "            dropout (float): Dropout 比例\n",
    "        \"\"\"\n",
    "        super(Transformer_encoder, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 输入嵌入层，将 delta_confidence 映射到嵌入空间\n",
    "        self.input_projection = nn.Linear(num_labels, embed_dim)\n",
    "\n",
    "        # Transformer 编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 聚合层，可以使用平均池化或其他策略\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # 输出层，将聚合后的表示映射到 delta_confidence\n",
    "        self.output_projection = nn.Linear(embed_dim, num_labels)\n",
    "\n",
    "    def positional_encoding(self, length, embed_dim, device):\n",
    "        \"\"\"\n",
    "        生成位置编码\n",
    "\n",
    "        参数:\n",
    "            length (int): 序列长度\n",
    "            embed_dim (int): 嵌入维度\n",
    "            device (torch.device): 设备（CPU/GPU）\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 位置编码，形状 (length, embed_dim)\n",
    "        \"\"\"\n",
    "        position = torch.arange(length, dtype=torch.float, device=device).unsqueeze(1)  # (length, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float, device=device) * \n",
    "                             -(math.log(10000.0) / embed_dim))  # (embed_dim // 2,)\n",
    "        pe = torch.zeros(length, embed_dim, device=device)  # (length, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "    def forward(self, support_deltas):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "            support_deltas (torch.Tensor): 支持示例的 delta_confidence，形状 (batch_size, k, num_labels)\n",
    "\n",
    "        返回:\n",
    "            delta_pred (torch.Tensor): 预测的查询示例的 delta_confidence，形状 (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # 支持示例嵌入\n",
    "        # 输入形状转换为 (k, batch_size, embed_dim) 以适应 Transformer 的输入要求\n",
    "        embedded = self.input_projection(support_deltas)  # (batch_size, k, embed_dim)\n",
    "        embedded = embedded.permute(1, 0, 2)  # (k, batch_size, embed_dim)\n",
    "\n",
    "        # 获取embeded第一个维度的长度\n",
    "        k = embedded.size(0)\n",
    "        pos_encoding = self.positional_encoding(k, self.embed_dim, device)  # (k, embed_dim)\n",
    "        embedded = embedded + pos_encoding.unsqueeze(1)  # 广播到 (k, batch_size, embed_dim)\n",
    "\n",
    "        # Transformer 编码\n",
    "        transformer_output = self.transformer_encoder(embedded)  # (k, batch_size, embed_dim)\n",
    "\n",
    "        # # 转回 (batch_size, embed_dim, k)\n",
    "        # transformer_output = transformer_output.permute(1, 2, 0)  # (batch_size, embed_dim, k)\n",
    "\n",
    "        # # 聚合，例如使用平均池化\n",
    "        # aggregated = self.pool(transformer_output).squeeze(-1)  # (batch_size, embed_dim)\n",
    "\n",
    "        last_hidden_state = transformer_output[-1, :, :]  # (batch_size, embed_dim)\n",
    "\n",
    "        # 输出层\n",
    "        delta_pred = self.output_projection(last_hidden_state)  # (batch_size, num_labels)\n",
    "\n",
    "        return delta_pred\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import math\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_labels, embed_dim=768, hidden_dim=1024, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        层级递归映射校准模型（基于RNN）\n",
    "\n",
    "        参数:\n",
    "            num_labels (int): 标签的数量\n",
    "            embed_dim (int): 嵌入维度\n",
    "            hidden_dim (int): RNN 隐藏状态的维度\n",
    "            num_layers (int): RNN 层数\n",
    "            dropout (float): Dropout 比例\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 输入嵌入层，将 delta_confidence 映射到嵌入空间\n",
    "        self.input_projection = nn.Linear(num_labels, embed_dim)\n",
    "        # 为输入添加 Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        # 使用 LSTM 作为递归神经网络（可以选择 GRU 或其他 RNN 变种）\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        # 输出层，将 RNN 最后一时间步的输出映射到 delta_confidence\n",
    "        self.output_projection = nn.Linear(hidden_dim, num_labels)\n",
    "        # 初始化权重\n",
    "        #self.init_lstm_weights()    def init_lstm_weights(self):\n",
    "        # LSTM层的权重初始化：正交初始化\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight_ih' in name:  # 输入门（input weights）\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:  # 隐藏门（hidden weights）\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:  # 偏置项初始化为零\n",
    "                init.zeros_(param.data)\n",
    "    def forward(self, support_deltas):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "            support_deltas (torch.Tensor): 支持示例的 delta_confidence，形状 (batch_size, k, num_labels)\n",
    "\n",
    "        返回:\n",
    "            delta_pred (torch.Tensor): 预测的查询示例的 delta_confidence，形状 (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # 支持示例嵌入\n",
    "        # 输入形状转换为 (batch_size, k, embed_dim) 以适应 LSTM 的输入要求\n",
    "        embedded = self.input_projection(support_deltas)  # (batch_size, k, embed_dim)\n",
    "\n",
    "        # Layer Normalization\n",
    "        normalized_embedded = self.layer_norm(embedded) # (batch_size, k, hidden_dim)\n",
    "\n",
    "        # LSTM 编码\n",
    "        rnn_output, _ = self.rnn(normalized_embedded)  # rnn_output: (batch_size, k, hidden_dim)\n",
    "\n",
    "        # 获取 RNN 输出的最后一个时间步的隐藏状态\n",
    "        last_hidden_state = rnn_output[:, -1, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        # 输出层\n",
    "        delta_pred = self.output_projection(last_hidden_state)  # (batch_size, num_labels)\n",
    "\n",
    "        return delta_pred\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, num_labels, embed_dim=768, hidden_dim=1024, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        层级递归映射校准模型（基于GRU）\n",
    "\n",
    "        参数:\n",
    "            num_labels (int): 标签的数量\n",
    "            embed_dim (int): 嵌入维度\n",
    "            hidden_dim (int): RNN 隐藏状态的维度\n",
    "            num_layers (int): RNN 层数\n",
    "            dropout (float): Dropout 比例\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 输入嵌入层，将 delta_confidence 映射到嵌入空间\n",
    "        self.input_projection = nn.Linear(num_labels, embed_dim)\n",
    "        # 为输入添加 Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 使用 GRU 作为递归神经网络（可以选择 GRU 或其他 RNN 变种）\n",
    "        self.rnn = nn.GRU(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_layers, \n",
    "                          dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 输出层，将 GRU 最后一时间步的输出映射到 delta_confidence\n",
    "        self.output_projection = nn.Linear(hidden_dim, num_labels, bias=True)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.init_gru_weights()\n",
    "\n",
    "    def init_gru_weights(self):\n",
    "        # GRU层的权重初始化：正交初始化\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight_ih' in name:  # 输入门（input weights）\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:  # 隐藏门（hidden weights）\n",
    "                init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:  # 偏置项初始化为零\n",
    "                init.zeros_(param.data)\n",
    "\n",
    "    def forward(self, support_deltas, p_pred_logits):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "            support_deltas (torch.Tensor): 支持示例的 delta_confidence，形状 (batch_size, k, num_labels)\n",
    "            p_pred_logits (torch.Tensor): 查询示例的预测置信度，形状 (batch_size, num_labels)\n",
    "\n",
    "        返回:\n",
    "            delta_pred (torch.Tensor): 预测的查询示例的 delta_confidence，形状 (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # 支持示例嵌入\n",
    "        # 输入形状转换为 (batch_size, k, embed_dim) 以适应 GRU 的输入要求\n",
    "        embedded = self.input_projection(support_deltas)  # (batch_size, k, embed_dim)\n",
    "\n",
    "        # Layer Normalization\n",
    "        normalized_embedded = self.layer_norm(embedded)  # (batch_size, k, embed_dim)\n",
    "\n",
    "        # GRU 编码\n",
    "        rnn_output, _ = self.rnn(normalized_embedded)  # rnn_output: (batch_size, k, hidden_dim)\n",
    "\n",
    "        # 获取 GRU 输出的最后一个时间步的隐藏状态\n",
    "        last_hidden_state = rnn_output[:, -1, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        # 输出层\n",
    "        prior_pred = self.output_projection(last_hidden_state)  # (batch_size, num_labels)\n",
    "\n",
    "        # 计算校准后的logits\n",
    "        cal_logits=p_pred_logits - prior_pred  # (batch_size, num_labels)\n",
    "\n",
    "        return cal_logits\n",
    "\n",
    "    \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_labels, embed_dim=768, hidden_dim=1024, num_layers=1, dropout=0):\n",
    "        \"\"\"\n",
    "        层级递归映射校准模型（基于普通RNN）\n",
    "\n",
    "        参数:\n",
    "            num_labels (int): 标签的数量\n",
    "            embed_dim (int): 嵌入维度\n",
    "            hidden_dim (int): RNN 隐藏状态的维度\n",
    "            num_layers (int): RNN 层数\n",
    "            dropout (float): Dropout 比例\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 输入嵌入层，将 delta_confidence 映射到嵌入空间\n",
    "        self.input_projection = nn.Linear(num_labels, embed_dim)\n",
    "        # 为输入添加 Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # 使用普通RNN\n",
    "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
    "                         dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 输出层，将RNN最后一时间步的输出映射到delta_confidence\n",
    "        self.output_projection = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, support_deltas):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "            support_deltas (torch.Tensor): 支持示例的 delta_confidence，形状 (batch_size, k, num_labels)\n",
    "\n",
    "        返回:\n",
    "            delta_pred (torch.Tensor): 预测的查询示例的 delta_confidence，形状 (batch_size, num_labels)\n",
    "        \"\"\"\n",
    "        # 支持示例嵌入\n",
    "        embedded = self.input_projection(support_deltas)  # (batch_size, k, embed_dim)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        normalized_embedded = self.layer_norm(embedded)   # (batch_size, k, embed_dim)\n",
    "        \n",
    "        # RNN编码\n",
    "        rnn_output, _ = self.rnn(normalized_embedded)     # rnn_output: (batch_size, k, hidden_dim)\n",
    "        \n",
    "        # 获取最后一个时间步的隐藏状态\n",
    "        last_hidden_state = rnn_output[:, -1, :]          # (batch_size, hidden_dim)\n",
    "        \n",
    "        # 输出层\n",
    "        delta_pred = self.output_projection(last_hidden_state)  # (batch_size, num_labels)\n",
    "        \n",
    "        return delta_pred\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, learning_rate, device, p_pred_train):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 模型\n",
    "        train_loader (DataLoader): 训练数据加载器\n",
    "        num_epochs (int): 训练轮数\n",
    "        learning_rate (float): 学习率\n",
    "        device (torch.device): 设备\n",
    "        p_pred_train (torch.Tensor): 训练集预测置信度，形状 (num_train, k+1, num_labels)\n",
    "\n",
    "    返回:\n",
    "        model: 训练后的模型\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        for batch in train_loader:\n",
    "            support_deltas, p_pred_query, query_deltas, p_true_query = batch\n",
    "            support_deltas = support_deltas.to(device)          # (batch_size, k, num_labels)\n",
    "            p_pred_query = p_pred_query.to(device)              # (batch_size, num_labels)\n",
    "            query_deltas = query_deltas.to(device)              # (batch_size, num_labels)\n",
    "            p_true_query = p_true_query.to(device)              # (batch_size, num_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            p_pred_logits = torch.log(p_pred_query)  # (batch_size, num_labels)\n",
    "\n",
    "            # 前向传播\n",
    "            delta_pred = model(support_deltas, p_pred_logits)                  # (batch_size, num_labels)\n",
    "\n",
    "            # 校准后的置信度\n",
    "            hat_p =  delta_pred                   # (batch_size, num_labels)\n",
    "\n",
    "            # 归一化 hat_p 使其成为有效的概率分布\n",
    "            #hat_p = hat_p / hat_p.sum(dim=1, keepdim=True)\n",
    "            #hat_p = hat_p / hat_p.norm(dim=1, keepdim=True)\n",
    "            #hat_p = F.softmax(hat_p, dim=1)\n",
    "            #hat_p = torch.logsumexp(hat_p, dim=1)\n",
    "\n",
    "            # 计算交叉熵损失\n",
    "            # 需要将 p_true_query 转换为类别标签\n",
    "            # p_true_query 是 one-hot 编码\n",
    "            _, targets = torch.max(p_true_query, dim=1)        # (batch_size)\n",
    "\n",
    "            loss = criterion(hat_p, targets)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "            continue\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 训练好的模型\n",
    "        test_loader (DataLoader): 测试数据加载器\n",
    "        device (torch.device): 设备\n",
    "\n",
    "    返回:\n",
    "        accuracy: 测试集上的准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            support_deltas, p_pred_query, query_deltas, p_true_query = batch\n",
    "            support_deltas = support_deltas.to(device)          # (batch_size, k, num_labels)\n",
    "            p_pred_query = p_pred_query.to(device)              # (batch_size, num_labels)\n",
    "            query_deltas = query_deltas.to(device)              # (batch_size, num_labels)\n",
    "            p_true_query = p_true_query.to(device)              # (batch_size, num_labels)\n",
    "\n",
    "\n",
    "            p_pred_logits = torch.log(p_pred_query)\n",
    "\n",
    "            # 前向传播\n",
    "            delta_pred = model(support_deltas,p_pred_logits)                  # (batch_size, num_labels)\n",
    "\n",
    "            # 校准后的置信度\n",
    "            hat_p =   delta_pred                   # (batch_size, num_labels)\n",
    "            #hat_p = torch.clamp(hat_p, min=1e-6, max=1.0) # 确保 hat_p 在有效范围内\n",
    "            #hat_p = hat_p / hat_p.sum(dim=1, keepdim=True)\n",
    "            #hat_p = hat_p / hat_p.norm(dim=1, keepdim=True)\n",
    "            hat_p = F.softmax(hat_p, dim=1)\n",
    "            #hat_p = torch.logsumexp(hat_p, dim=1)\n",
    "            # 预测类别\n",
    "            _, predicted = torch.max(hat_p, dim=1)              # (batch_size)\n",
    "\n",
    "            # 真实类别\n",
    "            _, targets = torch.max(p_true_query, dim=1)        # (batch_size)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            all_pred.append(hat_p.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    #print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy, all_pred\n",
    "\n",
    "def train_LinC_model(model, train_loader, num_epochs, learning_rate, device, p_pred_train):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 模型\n",
    "        train_loader (DataLoader): 训练数据加载器\n",
    "        num_epochs (int): 训练轮数\n",
    "        learning_rate (float): 学习率\n",
    "        device (torch.device): 设备\n",
    "        p_pred_train (torch.Tensor): 训练集预测置信度，形状 (num_train, k+1, num_labels)\n",
    "\n",
    "    返回:\n",
    "        model: 训练后的模型\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            support_deltas, p_pred_query, query_deltas, p_true_query = batch\n",
    "            support_deltas = support_deltas.to(device)          # (batch_size, k, num_labels)\n",
    "            query_deltas = query_deltas.to(device)              # (batch_size, num_labels)\n",
    "\n",
    "            p_pred_query = p_pred_query.to(device)              # (batch_size, num_labels)\n",
    "            p_true_query = p_true_query.to(device)              # (batch_size, num_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            hat_p = model(p_pred_query)                  # (batch_size, num_labels)\n",
    "            # 计算交叉熵损失\n",
    "            # p_true_query 是 one-hot 编码\n",
    "            _, targets = torch.max(p_true_query, dim=1)        # (batch_size)\n",
    "\n",
    "            loss = criterion(hat_p, targets)\n",
    "\n",
    "            # 反向传播与优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        if epoch % 10 == 0:\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "            continue\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_LinC_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    测试模型\n",
    "\n",
    "    参数:\n",
    "        model (nn.Module): 训练好的模型\n",
    "        test_loader (DataLoader): 测试数据加载器\n",
    "        device (torch.device): 设备\n",
    "\n",
    "    返回:\n",
    "        accuracy: 测试集上的准确率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            support_deltas, p_pred_query, query_deltas, p_true_query = batch\n",
    "            support_deltas = support_deltas.to(device)          # (batch_size, k, num_labels)\n",
    "            p_pred_query = p_pred_query.to(device)              # (batch_size, num_labels)\n",
    "            query_deltas = query_deltas.to(device)              # (batch_size, num_labels)\n",
    "            p_true_query = p_true_query.to(device)              # (batch_size, num_labels)\n",
    "\n",
    "            # 前向传播\n",
    "            hat_p = model(p_pred_query)                  # (batch_size, num_labels)\n",
    "\n",
    "            # 预测类别\n",
    "            _, predicted = torch.max(hat_p, dim=1)              # (batch_size)\n",
    "\n",
    "            # 真实类别\n",
    "            _, targets = torch.max(p_true_query, dim=1)        # (batch_size)\n",
    "\n",
    "            total += targets.size(0) \n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "class LinC(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(LinC, self).__init__()\n",
    "        self.classifier = nn.Linear(num_labels, num_labels, bias=True)\n",
    "    def forward(self, x):\n",
    "        clf_logits = self.classifier(x)\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dataset=MNLI, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8157 | train 93.4s /3.300GB | infer 0.02ms/样本 /1.098GB\n",
      "Running dataset=MNLI, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8154 | train 87.9s /3.300GB | infer 0.02ms/样本 /1.098GB\n",
      "Running dataset=MNLI, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8153 | train 99.6s /3.300GB | infer 0.02ms/样本 /1.098GB\n",
      "Running dataset=SST-2, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 23.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9511 | train 35.3s /1.206GB | infer 0.03ms/样本 /0.316GB\n",
      "Running dataset=SST-2, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9511 | train 36.4s /1.206GB | infer 0.02ms/样本 /0.316GB\n",
      "Running dataset=SST-2, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 21.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9511 | train 31.0s /1.206GB | infer 0.03ms/样本 /0.316GB\n",
      "Running dataset=MRPC, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7391 | train 25.9s /0.927GB | infer 0.03ms/样本 /0.307GB\n",
      "Running dataset=MRPC, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7391 | train 24.8s /0.927GB | infer 0.03ms/样本 /0.307GB\n",
      "Running dataset=MRPC, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7391 | train 23.0s /0.927GB | infer 0.03ms/样本 /0.307GB\n",
      "Running dataset=QNLI, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8041 | train 35.7s /1.206GB | infer 0.03ms/样本 /0.672GB\n",
      "Running dataset=QNLI, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8040 | train 29.3s /1.206GB | infer 0.04ms/样本 /0.672GB\n",
      "Running dataset=QNLI, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8041 | train 33.1s /1.206GB | infer 0.07ms/样本 /0.672GB\n",
      "Running dataset=RTE, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 148.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8375 | train 17.0s /0.683GB | infer 0.06ms/样本 /0.166GB\n",
      "Running dataset=RTE, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 127.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8375 | train 15.9s /0.683GB | infer 0.04ms/样本 /0.166GB\n",
      "Running dataset=RTE, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 132.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.8375 | train 16.8s /0.683GB | infer 0.04ms/样本 /0.166GB\n",
      "Running dataset=WiC, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 27.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.6271 | train 30.2s /1.295GB | infer 0.03ms/样本 /0.275GB\n",
      "Running dataset=WiC, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 30.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.6286 | train 37.3s /1.295GB | infer 0.03ms/样本 /0.275GB\n",
      "Running dataset=WiC, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.6293 | train 38.9s /1.295GB | infer 0.03ms/样本 /0.275GB\n",
      "Running dataset=YouTube, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 90.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9082 | train 10.0s /0.490GB | infer 0.04ms/样本 /0.177GB\n",
      "Running dataset=YouTube, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 99.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9107 | train 10.0s /0.490GB | infer 0.03ms/样本 /0.177GB\n",
      "Running dataset=YouTube, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 102.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.9107 | train 10.9s /0.490GB | infer 0.03ms/样本 /0.177GB\n",
      "Running dataset=AI-GA_1-1, seed=42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7971 | train 28.0s /1.064GB | infer 0.06ms/样本 /0.698GB\n",
      "Running dataset=AI-GA_1-1, seed=2025 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7976 | train 33.2s /1.064GB | infer 0.03ms/样本 /0.698GB\n",
      "Running dataset=AI-GA_1-1, seed=1234 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> acc=0.7982 | train 30.7s /1.064GB | infer 0.03ms/样本 /0.698GB\n",
      "All experiments done. Results saved to: monitoring_logs/monitor_qwen_7b_3shot_20250728_145132.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ──────────────── 数据集标签空间 ────────────────\n",
    "dataset_label = {\n",
    "    \"MNLI\":     ['no','maybe','yes'],\n",
    "    \"SST-2\":    ['negative', 'positive'],\n",
    "    \"MRPC\":     ['no','yes'],\n",
    "    \"QNLI\":     ['no','yes'],\n",
    "    \"RTE\":      ['no','yes'],\n",
    "    \"WiC\":      ['false', 'true'],\n",
    "    \"YouTube\":  ['truthful','deceptive'],\n",
    "    \"AI-GA_1-1\":['0', '1'],\n",
    "}\n",
    "\n",
    "# ──────────────── 实验配置 ────────────────\n",
    "seeds         = [42, 2025, 1234]\n",
    "datasets      = [\"MNLI\",\"SST-2\",\"MRPC\",\"QNLI\",\"RTE\",\"WiC\",\"YouTube\",\"AI-GA_1-1\"]\n",
    "shot          = \"3\"\n",
    "param         = \"7\"\n",
    "model_name    = \"qwen\"\n",
    "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs    = 200\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "# ──────────────── 工具函数 ────────────────\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # 保证 cudnn 的确定性\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "\n",
    "def reset_gpu_mem_stats():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "\n",
    "def get_gpu_peak():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.max_memory_allocated(device)\n",
    "    return 0\n",
    "\n",
    "\n",
    "# ──────────────── 单个实验 ────────────────\n",
    "def run_one(seed: int, dataset: str):\n",
    "    # 1. 固定随机种子\n",
    "    set_seed(seed)\n",
    "\n",
    "    # 2. 构造路径\n",
    "    train_data_path = (\n",
    "        f\"/workspace/ICL_calibration_v2/cache_0216/\"\n",
    "        f\"{dataset}/{model_name}_{param}b-top_bm25-{shot}increase-probe.jsonl\"\n",
    "    )\n",
    "    test_data_path = (\n",
    "        f\"/workspace/ICL_calibration_v2/output_0216/\"\n",
    "        f\"{dataset}/{model_name}_{param}b-top_bm25-{shot}increase.jsonl\"\n",
    "    )\n",
    "\n",
    "    # 3. 读取数据\n",
    "    label_space = dataset_label[dataset]\n",
    "    all_true_conf_train, all_pred_conf_train, _, _, _ = read_json(\n",
    "        train_data_path, label_space, sample_num=-1\n",
    "    )\n",
    "    all_true_conf_test,  all_pred_conf_test,  _, _, _ = read_json(\n",
    "        test_data_path, label_space\n",
    "    )\n",
    "\n",
    "    # 4. 构建 DataLoader\n",
    "    train_loader, test_loader = data_reader(\n",
    "        all_true_conf_train,\n",
    "        all_pred_conf_train,\n",
    "        all_true_conf_test,\n",
    "        all_pred_conf_test\n",
    "    )\n",
    "\n",
    "    # 5. 初始化模型\n",
    "    model = GRUModel(num_labels=len(label_space)).to(device)\n",
    "\n",
    "    # ≫ 训练阶段监控\n",
    "    reset_gpu_mem_stats()\n",
    "    train_start = time.time()\n",
    "\n",
    "    model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        num_epochs,\n",
    "        learning_rate,\n",
    "        device,\n",
    "        all_pred_conf_train\n",
    "    )\n",
    "\n",
    "    train_elapsed   = time.time() - train_start\n",
    "    train_gpu_peak  = get_gpu_peak()\n",
    "\n",
    "    # ≫ 推理阶段监控\n",
    "    reset_gpu_mem_stats()\n",
    "    infer_start = time.time()\n",
    "\n",
    "    accuracy, _ = test_model(model, test_loader, device)\n",
    "\n",
    "    infer_elapsed   = time.time() - infer_start\n",
    "    infer_gpu_peak  = get_gpu_peak()\n",
    "    infer_avg_time  = infer_elapsed / len(test_loader.dataset)\n",
    "\n",
    "    metrics = {\n",
    "        # 训练\n",
    "        \"train_time_s\":          train_elapsed,\n",
    "        \"train_gpu_peak_bytes\":  train_gpu_peak,\n",
    "        # 推理\n",
    "        \"infer_time_total_s\":    infer_elapsed,\n",
    "        \"infer_time_per_sample_s\": infer_avg_time,\n",
    "        \"infer_gpu_peak_bytes\":  infer_gpu_peak,\n",
    "    }\n",
    "    return accuracy, metrics\n",
    "\n",
    "\n",
    "# ──────────────── 主流程 ────────────────\n",
    "def main():\n",
    "    results = {}\n",
    "    for ds in datasets:\n",
    "        accs = []\n",
    "        metrics_all = []\n",
    "        for sd in seeds:\n",
    "            print(f\"Running dataset={ds}, seed={sd} ...\")\n",
    "            acc, m = run_one(sd, ds)\n",
    "            print(\n",
    "                f\"  -> acc={acc:.4f} | \"\n",
    "                f\"train {m['train_time_s']:.1f}s /\"\n",
    "                f\"{m['train_gpu_peak_bytes']/1e9:.3f}GB | \"\n",
    "                f\"infer {m['infer_time_per_sample_s']*1e3:.2f}ms/样本 /\"\n",
    "                f\"{m['infer_gpu_peak_bytes']/1e9:.3f}GB\"\n",
    "            )\n",
    "            accs.append(acc)\n",
    "            metrics_all.append(m)\n",
    "\n",
    "        results[ds] = {\n",
    "            \"accuracies\":      accs,\n",
    "            \"mean_accuracy\":   float(np.mean(accs)),\n",
    "            \"std_accuracy\":    float(np.std(accs, ddof=1)),\n",
    "            \"metrics\":         metrics_all\n",
    "        }\n",
    "\n",
    "    # 生成带时间戳的监控文件名\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir   = Path(\"monitoring_logs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    outfile   = out_dir / f\"monitor_{model_name}_{param}b_{shot}shot_{timestamp}.json\"\n",
    "\n",
    "    with outfile.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"All experiments done. Results saved to: {outfile}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
